-- Στοιχεια φοιτητων --

Ανδρεας κοκκορας sdi2000085

Νικος μιχαλουτσος sdi2000133

Τροπος μεταγλωτισης lsh: make lsh ή g++ -o lsh lsh.cpp lsh_func.cpp generals.cpp
Τροπος μεταγλωτισης cube: make cube ή g++ -o cube cube.cpp lsh_func.cpp generals.cpp
Τροπος μεταγλωτισης cluster: make cluster ή g++ -o cluster cluster.cpp cluster_func.cpp cube_func.cpp lsh_func.cpp generals.cpp
Τροπος μεταγλωτισης graph_search: make graph_search ή g++ -pthread -o graph_search graph_search.cpp graph_search_func.cpp generals.cpp lsh_func.cpp

------------------------------------------------------------------------- Εργασια 3 ----------------------------------------------------------------------------------------

Ερωτηματα που εγιναν: Α, Β.

Τα προγραμματα δεχονται ως εισοδο πλεον τα encoded αρχεια! ΟΧΙ τα κανονικα.

-- Περιγραφη προγραμματος --
- Το προγραμμα αυτο εχει στοχο τη διανυσματικη αναπαρασταση εικονας σε χωρο χαμηλοτερης διαστασης με τη χρηση αυτοκωδικοποιητη, κι υστερα την συγκριση αποστασεων και συσταδοποιηση των διανυσματων. Το προγραμμα με python φτιαχνει εναν αυτοκωδικοιητη. Και πλεον προσαρμοζει τα προγραμματα ευρεσης κοντινοτερου γειτονα με διαφορες μεθοδους στο να λειτουργουν κι αυτα στη χαμηλοτερη διασταση του αυτοκωδικοποιητη.

-- Αρχεια κωδικα και περιγραφη τους --
reduce.py: Με tensorflow και keras και την βοηθεια των νευρωνικων δικτυων χτιζει εναν αποκωδικοποιητη για την αναπαρασταση των εικονων σε 10 διαστασεις. Αρχικα, προτιμηθηκε Sequential μοντελο επειδη το προβλημα πιστευω πως θεωρειται απλο και δεν υπαρχει αναγκη για το flexibility που δινει το functional. Παραλληλα κερδιζει σε απλοτητα. Το μοντελο εχει βελτιστοποιηθει υστερα απο πολλα πειραματα, και χρησιμοποιει optimal επιλογες μες στο μοντελο. Το 10% των δεδομενων επιλεχθηκε για validation, και το υπολοιπο για testing.

-- Σε ΟΛΑ τα υπολοιπα αρχεια (εκτος cluster μιας και το ερωτημα Γ δεν εγινε) πλεον εχει γινει η εξης αλλαγη: διαβαζει τα encoded_input και encoded_query πλεον, και ΟΧΙ τα input και query στον κανονικο χωρο. Η εκφωνηση δεν ειχε ξεκαθαρο το τι να κανουμε οποτε προτιμησα αυτο. Για τον λογο αυτον το κανονικο dataset file πρεπει υποχρεωτικα να λεγεται "input.dat" και το κανονικο queryset file πρεπει να λεγεται query.dat.

-- Πειραματα --
Παραθετω παρακατω τα πειραματα που καναμε:

Αρχικα εγιναν πειραματα με τα Epochs και τα Batch Sizes:

Optimizer   |   Loss Func   |   Epochs  |   Batch   |   Layers  |   Filters |   Act. Func   |   MRNG    |   GNNS    |   LSH     |   Cube    | Validation Loss
    Adam    | Mean Squared  |     5     |    128    |     3     |      16   |      Relu     | AAF = 2.1 | AAF = 2.2 | MAF = 3.0 | MAF = 6.0 |    0.0157
    Adam    | Mean Squared  |     5     |    500    |     3     |      16   |      Relu     | AAF = 2.3 | AAF = 3.5 | MAF = 2.2 | MAF = 9.0 |    0.0191
    Adam    | Mean Squared  |     5     |   1000    |     3     |      16   |      Relu     | AAF = 2.2 | AAF = 2.4 | MAF = 7.0 | MAF = 3.1 |    0.0233
    Adam    | Mean Squared  |    10     |    128    |     3     |      16   |      Relu     | AAF = 3.0 | AAF = 2.7 | MAF = 7.8 | MAF = 6.2 |    0.0142
    Adam    | Mean Squared  |    10     |    500    |     3     |      16   |      Relu     | AAF = 2.7 | AAF = 3.4 | MAF = 8.0 | MAF = 6.0 |    0.0187
    Adam    | Mean Squared  |    10     |   1000    |     3     |      16   |      Relu     | AAF = 3.4 | AAF = 3.1 | MAF = 9.0 | MAF = 7.8 |    0.0210
    Adam    | Mean Squared  |     2     |    128    |     3     |      16   |      Relu     | AAF = 3.3 | AAF = 3.2 | MAF = 9.2 | MAF = 4.0 |    0.0208
    Adam    | Mean Squared  |     2     |    500    |     3     |      16   |      Relu     | AAF = 4.3 | AAF = 4.0 | MAF = 6.0 | MAF = 4.5 |    0.0251
    Adam    | Mean Squared  |     2     |   1000    |     3     |      16   |      Relu     | AAF = 4.6 | AAF = 4.3 | MAF = 7.2 | MAF = 7.0 |    0.0353

Παρατηρειται οτι τα 5 Epochs ειναι αρκετα κι αποφευγουν τοσο το overfitting οσο και το underfitting. Στα 10 epochs εχουμε χειροτερα αποτελεσματα στις αναζητησεις, μαλλον λογω overfitting. Επισης, χειροτερα τα αποτελεσματα με 2 epochs, μαλλον λογω underfitting. Το batch size στα 128 με 5 epochs παρατηρηθηκε οτι βελτιστοποιει το prediction μαλλον επειδη με λιγοτερο batch size αλλαζουν συχνοτερα τα βαρη αρα δινεται η ευκαιρια στον αλγοριθμο να κανει πιο συχνα τις καταλληλες βελτιωσεις. Επισης με τα 10 epochs το training παιρνει πολυ περισσοτερη ωρα ενω στα 2 epochs παιρνει λιγοτερη.

Επειτα με 5 epochs και 128 batch size, εγιναν πειραματα στον αριθμο στρωματων (layers):

Optimizer   |   Loss Func   |   Epochs  |   Batch   |   Layers  |   Filters |   Act. Func   |   MRNG    |   GNNS    |   LSH     |   Cube    | Validation Loss
    Adam    | Mean Squared  |     5     |    128    |     3     |      16   |      Relu     | AAF = 2.1 | AAF = 2.2 | MAF = 3.0 | MAF = 6.0 |    0.0157
    Adam    | Mean Squared  |     5     |    128    |     2     |      16   |      Relu     | AAF = 3.1 | AAF = 3.4 | MAF = 6.0 | MAF = 6.8 |    0.0195

Δεν βγαινει ο αλγοριθμος με 4 layers διοτι η διασταση μας ειναι 28x28. Αν βαλουμε 4 layers τοτε στον maxpooling του encoder θα γινει 14x14, επειτα 7x7, και μετα 3x3. Και στο upsampling του decoder θα γινει 6x6, μετα 12x12, και μετα 24x24. Οποτε το training δεν μπορει να γινει. Παρολα αυτα αν επιλεξουμε αναμεσα σε 2 ή 3 layers θα προτιμησουμε ξεκαθαρα τα 3. Οσον αφορα τον χρονο του training δεν υπαρχουν μεγαλες αλλαγες, ειναι λιγο πιο γρηγορο με τα 2 layers.

Συνεχιζοντας με 5 epochs, 128 batch size και 3 στρωματα λοιπον, εγιναν πειραματα στον αριθμο φιλτρων ανα στρωμα:

Optimizer   |   Loss Func   |   Epochs  |   Batch   |   Layers  |   Filters |   Act. Func   |   MRNG    |   GNNS    |   LSH     |   Cube    | Validation Loss
    Adam    | Mean Squared  |     5     |    128    |     3     |      16   |      Relu     | AAF = 2.1 | AAF = 2.2 | MAF = 3.0 | MAF = 6.0 |    0.0157
    Adam    | Mean Squared  |     5     |    128    |     3     |      8    |      Relu     | AAF = 3.8 | AAF = 4.6 | MAF = 8.0 | MAF = 7.5 |    0.0220
    Adam    | Mean Squared  |     5     |    128    |     3     |      32   |      Relu     | AAF = 2.1 | AAF = 2.2 | MAF = 3.0 | MAF = 6.0 |    0.0214

Αν και δεν ανεμενα τετοια διαφορα στην αποτελεσματικοτητα, κιομως ο αριθμος φιλτρων ανα στρωμα μπορει να κανει μεγαλη διαφορα στο ποση πληροφορια κραταει ανα στρωμα το μοντελο, και αρα στο αποτελεσμα που θα εχουμε στο τελος. Αξιζει επισης να σημειωθει οτι παρατηρηθηκε μεγαλη διαφορα και στον χρονο που απαιτηθηκε για να κανει training το μοντελο αφου στο μοντελο με τα 8 φιλτρα ανα στρωμα πηρε μονο 86 δευτερολεπτα, στο μοντελο με τα 16 φιλτρα ανα στρωμα πηρε 167 δευτερολεπτα (σχεδον το διπλασιο), και στο μοντελο με τα 32 φιλτρα ανα στρωμα πηρε 358 δευτερολεπτα (λιγο παραπανω απ το διπλασιο του μοντελου με τα 16). Παρολα αυτα, στο μοντελο με τα 32 φιλτρα ανα στρωμα δεν εχουμε τοσο καλα αποτελεσματα οσο σε αυτο με τα 16, πιθανον και λογω overfitting. Αρα θα συνεχισουμε με τα 16.

Αφου λοιπον κραταμε 5 epochs, 128 batch size, 3 στρωματα και 16 φιλτρα ανα στρωμα, εγιναν πειραματα με τα Activation functions:

Optimizer   |   Loss Func   |   Epochs  |   Batch   |   Layers  |   Filters |   Act. Func   |   MRNG    |   GNNS    |   LSH     |   Cube    | Validation Loss
    Adam    | Mean Squared  |     5     |    128    |     3     |      16   |      Relu     | AAF = 2.1 | AAF = 2.2 | MAF = 3.0 | MAF = 6.0 |    0.0157
    Adam    | Mean Squared  |     5     |    128    |     3     |      16   |   LeakyRelu   | AAF = 2.5 | AAF = 2.8 | MAF = 7.2 | MAF = 9.0 |    0.0180

Εφοσον με relu ηταν καλυτερο απο οτι με LeakyRelu, εγιναν πειραματα με την activation function του τελευταιου layer του decoder, χρησιμοποιωντας sigmoid:

Optimizer   |   Loss Func   |   Epochs  |   Batch   |   Layers  |   Filters |   Act. Func   |   MRNG    |   GNNS    |   LSH     |   Cube    | Validation Loss
    Adam    | Mean Squared  |     5     |    128    |     3     |      16   |  Relu-Sigmoid | AAF = 3.4 | AAF = 2.5 | MAF = 4.0 | MAF = 10.0 |    0.1122

Παρατηρειται πολυ χειροτερο Validation loss και στα πειραματα χειροτερα αποτελεσματα απο της σκετης Relu. Αρα κραταμε την Relu.

Επειτα, αφου κραταμε ολα οσα θεωρουμε βελτιστα ως τωρα συνεχιζουμε με πειραματα στον optimizer:

Optimizer   |   Loss Func   |   Epochs  |   Batch   |   Layers  |   Filters |   Act. Func   |   MRNG    |   GNNS    |   LSH     |   Cube    | Validation Loss
    Adam    | Mean Squared  |     5     |    128    |     3     |      16   |      Relu     | AAF = 2.1 | AAF = 2.2 | MAF = 3.0 | MAF = 6.0 |    0.0157
   RMSprop  | Mean Squared  |     5     |    128    |     3     |      16   |      Relu     | AAF = 3.5 | AAF = 2.4 | MAF = 9.7 | MAF = 10  |    0.0191
 Stochastic | Mean Squared  |     5     |    128    |     3     |      16   |      Relu     | AAF = 2.3 | AAF = 2.1 | MAF = 3.8 | MAF = 10  |    0.0361

Παρατηρειται οτι η Adam πραγματι, ειναι καλυτερη συγκριτικα με τις υπολοιπες μεθοδους τοσο σε validation loss οσο και σε αποτελεσματα στα πειραματα.

Τελος, πραγματοποιηθηκε ενα πειραμα με το latent dimension να ειναι 30:

Optimizer   |   Loss Func   |   Epochs  |   Batch   |   Layers  |   Filters |   Act. Func   |   MRNG    |   GNNS    |   LSH     |   Cube    | Validation Loss
    Adam    | Mean Squared  |     5     |    128    |     3     |      16   |      Relu     | AAF = 2.0 | AAF = 2.1 | MAF = 5.8 | MAF = 7.3 |    0.1122

Μου κανει εντυπωση που τα αποτελεσματα στα πειραματα με τις προσεγγιστικες αναζητησεις δεν μοιαζουν να εχουν μεγαλη διαφορα, μαλιστα αν εξαιρεσουμε τους αλγοριθμους που χρησιμοποιουνε γραφους, στους αλλους 2 εβγαλε χειροτερα αποτελεσματα. Περισσοτερη εντυπωση ομως μου κανει το Validation loss το οποιο ειναι σχεδον 10πλασιο απο τις 10 διαστασεις. Ισως να κρατειτε παραπανω πληροφορια απο οσο πρεπει κι αυτο να οδηγει σε overfitting. Προσωπικα, μου φανηκε το πιο ενδιαφερον μερος των πειραματων.

-- Οδηγιες χρησης του προγραμματος --
Αυτοκωδικοποιητης: python3 reduce.py -d <dataset> -q <queryset> -od <dataset_output> -oq <queryset_output>
Τα υπολοιπα προγραμματα οπως περιγραφονται στις παλαιοτερες εργασιες με την διαφορα οτι τα αρχεια που δεχεται input ειναι του νεου χωρου κι ΟΧΙ του παλιου.

------------------------------------------------------------------------- Εργασια 2 ----------------------------------------------------------------------------------------

-- Περιγραφη προγραμματος --
- Το προγραμμα αυτο στοχο εχει την εφαρμογη των αλγοριθμων GNNS, MRNG για κατασκευη γραφων ευρεσης κοντινοτερου γειτονα και του αλγοριθμου Search On Graph για την αναζητηση κοντινοτερου γειτονα σε γραφο, σε δεδομενα που στη συγκεκριμενη περιπτωση ειναι εικονες, απεικονισμενες ως διανυσματα. Το προγραμμα δημιουργει εναν γραφο οπου το καθε image αναπαριστα εναν κομβο και αρχιζοντας απο εναν κομβο συμφωνα με τον αλγοριθμο, βρισκει το 'κοντινοτερο' image προς το query. Ο χρηστης δινει ενα συνολο dataset κι ενα συνολο αναζητησης, το προγραμμα παιρνει μια εικονα απο το συνολο αναζητησης, εφαρμοζει μια απο τις 2 μεθοδους γραφων την οποια επιλεγει ο χρηστης και βρισκει την πιο παρομοια εικονα απο το dataset στο query. Εξαγει ενα αρχειο στο οποιο εκτυπωνεται η μεθοδος που επιλεχθηκε, οι προσεγγιστικα κοντινοτεροι γειτονες του καθε query, οι πραγματικα κοντινοτεροι γειτονες στο query, οι μεσοι οροι χρονου ευρεσης αυτων, και το μεγιστο κλασμα προσεγγισης αυτων.

GNNS: Ακολουθησαμε τις διαφανειες και απλα κατασκευαζουμε τον γραφο οπως λενε οι διαφανειες, κανοντας lsh για καθε σημειο και βαζοντας του ακμες προς τα αποτελεσματα της lsh. Υστερα κανουμε αναζητηση παλι οπως λενε οι διαφανειες, χρησιμοποιωντας vector αντι για set στο S, και βρισκοντας τον κοντινοτερο μεσω της κλασης Neibs στο τελος.

MRNG: Καναμε μικρες αλλαγες για την επιταχυνση του αλγοριθμου κατασκευης γιατι ο τροπος των διαφανειων ειναι απιστευτα αργος. Υπολογιζουμε μια φορα σε εναν πινακα τις αποστασεις του καθε σημειου με καθε αλλο πριν την κατασκευη του γραφου με optimal τροπο τοσο απο αποψης time complexity οσο και απο space complexity, ωστε να μην το υπολογιζουμε συνεχεια στην κατασκευη του γραφου. Χρησιμοποιωντας unordered_set για το Rp, και ordered_set για το Lp, υπολογιζουμε το Rp μια φορα πριν αρχισουμε την κατασκευη και του βαζουμε ολα τα σημεια ωστε να χρειαζεται μονο να του βγαζουμε προσωρινα καθε φορα τα Lp σημεια μες στην επαναληψη της κατασκευης, πετυχαινοντας λιγοτερες ενεργειες επιταχυνοντας τη μεθοδο. Οσο για την Search On Graph το l το οριζουμε ως το πληθος των στοιχειων που ελεγξαμε απο το συνολο, κι οχι το πληθος στοιχειων που βαλαμε σε αυτο (με λιγα λογια ακολουθησαμε το paper γιατι ηταν το ιδιο γρηγορο αλλα πιο αποτελεσματικο). Τελος, χρησιμοποιησαμε threads για την επιταχυνση του αλγοριθμου κι αποδειχθηκε σωστη επιλογη σε εμας.

-- Αρχεια Κωδικα και Περιγραφη τους --
- graph_search.cpp: Το αρχείο αυτό περιέχει τον κώδικα κατασκευης γραφων για την GNNS και MRNG και στη συνεχεια αλγοριθμο αναζητησης Search On Graph. Διαβαζει απο την εισοδο τα αρχεια εισοδου εξοδου και αναζητησης καθως και ορισματα τα οποια επηρεαζουν τη συμπεριφορα της αναζητησης καθος και τη μεθοδο κατασκευης γραφου κι αναζητησης. Διαβαζει το αρχειο του dataset. Φτιαχνει τον γραφο συμφωνα με την επιλεγμενη μεθοδο, και φτιαχνει και τον αρχικο κομβο αν η επιλεγμενη μεθοδος ειναι η MRNG. Υστερα ζηταει το αρχειο αναζητησης και το αρχειο εξοδου αν δεν εχει ηδη δοθει εξ αρχης. Κανει την αναζητηση με την Search on Graph αν εχει επιλεχθει η MRNG, ή με τον τροπο της GNNS κι εξαγει αποτελεσματα στο αρχειο εξοδου.

- graph_search_func.cpp: Περιεχει βοηθητικες συναρτησεις που χρησιμοποιει το graph_search.cpp, εκτος απο αυτες που χρησιμοποιουν templates, οι οποιες περιεχονται στην graph_search_func.h. Περιεχει συναρτησεις οπως την min() η οποια χρησιμοποιειται στην gnns_search(), την end_init η οποια συμβαλει στον χωρισμο του πινακα αποστασεων d της mrng σε numThreads, την getIndex() η οποια χρησιμοποιειται για να απεικονισουμε τον δισδιαστατο πινακα αποστασεων d σε μονοδιαστατο γλυτωνοντας πολυ χωρο, τις υλοποιησεις των συναρτησεων του γραφου που χρησιμοποιουμε και την set_insert που χρησιμοποιειται στην gnns_search().

- graph_search_func.h: Περιεχει τις συναρτησεις οι οποιες χρησιμοποιουν template και χρησιμοποιουνται απο την graph_search.cpp. Οι βασικοτερες ειναι η gnns_construction() για την κατασκευη του GNNS γραφου, η gnns_search() για την αναζητηση στον γραφο αυτον, η threaded mrng() οι οποια χρησιμοποιωντας threads βελτιωνει κατα πολυ τον χρονο της υψηλης σε πολυπλοκοτητα mrng() η οποια κατασκευαζει τον mrng γραφο, η search_on_graph() για την αναζητηση στον MRNG γραφο, η threaded_distances_init() η οποια χρησιμοποιωντας μαθηματικες εξισωσεις και με τη βοηθεια της end_init(), χωριζει σε δικαια τμηματα τον πινακα αποστασεων d σε threads ωστε να επιταχυνθει τη κατασκευη του, καθως και την distance_init που χρησιμοποιει καθε thread για την κατασκευη ενος μερους του πινακα d.

-- Oδηγίες χρήσης του προγράμματος --

Γινεται εισαγωγη ενος αρχειου input.dat που περιεχει εικονες αριθμητικων
ψηφιων 28*28 pixels που εχουν ακαιρεα τιμη απο 0 μεχρι 255.

Οπως και ενα αρχειο query.dat με το συνολο αναζητησης ιδιου τυπου με το input.dat.

Αρχικα ο χρηστης εισαγει το μονοπατι του dataset και μετα το μονοπατι του αρχειου αναζητησης και εξοδου, αφου το προγραμμα παραγει αποτελεσματα εχει την επιλογη να τερματισει το προγραμμα ή να επαναλαβει την αναζητηση με αλλο αρχειο, κι εχουμε δωσει και την επιπλεον επιλογη με σκετο enter να ξανατρεχει με το ιδιο αρχειο αναζητησης.

Ο χρηστης εχει την επιλογη να δωσει τρεξει το προγραμμα με τις προεπιλεγμενες τιμες των μεταβλητων που επηρεαζουν τη συμπεριφορα του προγραμματος. Μπορει ομως να δωσει τις δικες του τιμες για τις μεταβλητες R (αριθμος τυχαιων επανεκκινησεων gnns), την Ε (αριθμος επεκτασεων), την Ν (αριθμος πλησιεστερων γειτονων του αλγοριθμου), και την k (αριθμος γειτονων του καθε κομβου στον γραφο) οσο αφορα τη μεθοδο GNNS. Για τη μεθοδο MRNG μπορει να δωσει τιμες στα ορισματα k οπου παλι ειναι ο αριθμος των γειτονων του καθε κομβου στον γραφο, και του ορισματος l οπου ειναι ο αριθμος στοιχειων που θα ελεγξουμε για γειτονες στην search on graph. (Το l σε μας αναπαριστα το ποσα στοιχεια τσακαραμε απο το συνολο S κι οχι το ποσα στοιχεια βαλαμε σε αυτο. Εχουμε ακολουθησει την μεθοδο του paper κι οχι των διαφανειων διοτι ηταν πολυ πιο αποτελεσματικη).

- Τα αρχεια εισοδου και αναζητησης θα μπορουν να δινονται και μεσω παραμετρων στη γραμμη εντολων, αν οχι, τοτε ο χρηστης τα εισαγει κατα την εκτελεση του προγραμματος.
Οποτε η εκτελεση θα γινεται μεσω της εντολης:

./graph_search –d <input file> –q <query file> –k <int> -E <int> -R <int> -N <int> -l <int, only for Search-on-Graph> -m <1 for GNNS, 2 for MRNG> -ο <outputfile>

Συγκριση αλγοριθμων (Ερωτημα Γ):

Κι οι 2 αλγοριθμοι γενικοτερα βγαζουν καλα αποτελεσματα αναλογως τα ορισματα που τους δινονται. Ο καθενας εχει τα πλεονεκτηματα του και τα μειονεκτηματα του τοσο στα αποτελεσματα οσο και στον χρονο, στη χρηση πορων που απαιτουν και την απλοτητα τους.
Ο GNNS ειναι πιο γρηγορος στην κατασκευη του απο τον MRNG. Παρολα αυτα ο MRNG ειναι αναμφισβητητα πιο αποτελεσματικος αφου κατασκευαστει.

Πειραματα με GNNS:
k = 50  | E = 30 | R = 1 | N = 1 | local_optimal = false    | => MAF = 4.7
k = 50  | E = 30 | R = 1 | N = 1 | local_optimal = true     | => MAF = 5.2
k = 50  | E = 30 | R = 3 | N = 1 | local_optimal = true     | => MAF = 3.7
k = 50  | E = 30 | R = 3 | N = 1 | local_optimal = false    | => MAF = 2.9
k = 50  | E = 30 | R = 6 | N = 1 | local_optimal = true     | => MAF = 2.6
k = 50  | E = 30 | R = 6 | N = 1 | local_optimal = false    | => MAF = 2.05
k = 70  | E = 50 | R = 1 | N = 1 | local_optimal = false    | => MAF = 3.5
k = 100 | E = 50 | R = 1 | N = 1 | local_optimal = false    | => MAF = 3.7
k = 100 | E = 50 | R = 6 | N = 1 | local_optimal = false    | => MAF = 3.6
k = 40  | E = 40 | R = 10 | N = 1 | local_optimal = false    | => MAF = 1.42    | tApprox = 0.2 secs
k = 40  | E = 1 | R = 10 | N = 1 | local_optimal = false    | => MAF = 2.62     | tApprox = 0.006 secs
k = 40  | E = 1 | R = 2 | N = 1 | local_optimal = false    | => MAF = 6.6       | tApprox = 0.001 secs

Γενικοτερα οσο πιο γρηγορος ειναι τοσο μη αποτελεσματικος ειναι ο αλγοριθμος. Μπορει να επιλεξει καποιος να ειναι γρηγορος με χαμηλο αριθμο E, R, ή και k, και να μην παρει τοσο καλα αποτελεσματα. Αν ομως βαλει πιο υψηλους αριθμους και ειδικα στο R τοτε θα παρει καλυτερα. Παρολα αυτα οχι τοσο καλα οσο του MRNG. Το local optimal σε γενικες γραμμες δεν βοηθαει τις περισσοτερες φορες, αλλα επιταχυνει ελαχιστα τον αλγοριθμο καποιες φορες. Μπορει στο dataset το δικο μας να μην συμφερει κατα τη γνωμη μας αλλα ισως σε ακομη μεγαλυτερα, να φανει χρησιμο.

Πειραματα με MRNG:
k = 50  | l = 2     | => MAF = 3.48    | tApprox = 0.01 secs
k = 50  | l = 3     | => MAF = 2.75    | tApprox = 0.02 secs
k = 50  | l = 4     | => MAF = 2.75    | tApprox = 0.03 secs
k = 50  | l = 5     | => MAF = 1.20    | tApprox = 0.03 secs
k = 50  | l = 6     | => MAF = 1.12    | tApprox = 0.05 secs
k = 50  | l = 7     | => MAF = 1.08    | tApprox = 0.05 secs
k = 50  | l = 8     | => MAF = 1.00    | tApprox = 0.06 secs

Μετα το l = 8 βρισκει παντοτε τον πραγματικα κοντινοτερο γειτονα σε πολυ καλους χρονους. Αξιζει να ανεβασουμε το l στο δικο μας dataset διοτι για πολυ μικρη θυσια χρονου περνουμε πολυ καλυτερα αποτελεσματα, παρολα αυτα σε ενα μεγαλυτερο δεν γνωριζουμε κατα ποσο θα ισχυε αυτο. Αν εξερεσουμε το αρνητικο του μεγαλου χρονου κατασκευης και υψηλης χρησης πορων, ο αλγοριθμος ειναι γρηγορος και πολυ πιο αποτελεσματικος.

------------------------------------------------------------------------- Εργασια 1 ----------------------------------------------------------------------------------------

-- Περιγραφη προγραμματος --
- Το προγραμμα αυτο στοχο εχει την εφαρμογη της μεθοδου LSH knn, LSH Range Search, Hypercube knn, Hypercube Range Search σε δεδομενα οπου στην προκειμενη περιπτωση ειναι images. Το προγραμμα συγκρινει εικονες μεταξυ τους και βλεπει ποια εικονα ειναι πιο 'κοντα' στην εικονα query. Ο χρηστης δινει ενα συνολο dataset κι ενα συνολο αναζητησης, το προγραμμα παιρνει μια εικονα απο το συνολο αναζητησης, εφαρμοζει τις μεθοδους και βρισκει την πιο παρομοια εικονα απο το dataset σε αυτην. Εκτυπωνει τις εικονες αυτες που βρηκε, την 'αποσταση' της απο την query εικονα, και την αποσταση της πραγματικα κοντινοτερης εικονας ως ενδειξη της αποτελεσματικοτητας της μεθοδου. 

-- Αρχεια Κωδικα και Περιγραφη τους --
- lsh.cpp: Το αρχειο αυτο περιεχει τον κωδικα για την lsh knn και την lsh Range Search. Αρχικα διαβαζει απο την εισοδο αν εχει εισαγει ο χρηστης δεδομενα οπως ονοματα αρχειων ή τιμες μεταβλητων με τον τροπο που λεει η εκφωνηση, αλλιως χρησιμοποιει τις προεπιλογες που προτεινονται. Υστερα διαβαζει αρχειο του dataset. Φτιαχνει τη δομη αναζητησης και στη συνεχεια ζηταει το αρχειο αναζητησης και το αρχειο εξοδου. Κανει την αναζητηση με τον τροπο της LSH, τοσο του knn οσο και του Range Search, και εξαγει τα αποτελεσματα στο αρχειο εξοδου. Ελευθερωνει τη μνημη και τερματιζεται.

- cube.cpp: Το αρχειο αυτο περιεχει τον κωδικα για την hypercube knn και την hypercube Range Search. Αρχικα διαβαζει απο την εισοδο αν εχει εισαγει ο χρηστης δεδομενα οπως ονοματα αρχειων ή τιμες μεταβλητων με τον τροπο που λεει η εκφωνηση, αλλιως χρησιμοποιει τις προεπιλογες που προτεινονται. Υστερα διαβαζει αρχειο του dataset. Φτιαχνει τη δομη αναζητησης και στη συνεχεια ζηταει το αρχειο αναζητησης και το αρχειο εξοδου. Κανει την αναζητηση με τον τροπο της Hypercube knn και Range Search, και εξαγει τα αποτελεσματα στο αρχειο εξοδου. Ελευθερωνει τη μνημη και τερματιζεται.

- lsh_func.cpp: Το αρχειο αυτο περιεχει 2 συναρτησεις τις LSH μεθοδου, την h και την g. Οι συναρτησεις αυτες εχουν ως λειτουργια να παραγουν ακεραιους με καποια δεδομενα ως εισοδο.

- generals.cpp: Το αρχειο αυτο περιεχει συναρτησης γενικων χρησεων οπως η συναρτηση για την αναγνωση αρχειου readfile, η συναρτηση για υπολογισμο αποστασης k-μετρικης (dist), και μια κλαση γενικης χρησης Neibs η οποια εχει τα δικα της δεδομενα και λειτουργιες. Εκτος απο constructor/destructor η Neibs εχει μια πολυ βασικη συναρτηση insertion_sort η οποια εισαγει ταξινομημενους ακεραιους αριθμους στον πινακα nn της κλασης, και απο κει και περα εχει συναρτησης για αναγνωση δεδομενων της οπως η givenn(), givenn(int), give_size(), give_dist().

-- Oδηγίεσ χρήσης του προγράμματος --

Γινεται εισαγωγη ενος αρχειου input.dat που περιεχει εικονες αριθμητικων
ψηφιων 28*28 pixels που εχουν ακαιρεα τιμη απο 0 μεχρι 255.

Οπως και ενα αρχειο query.dat με το συνολο αναζητησης και εχει τουλαχιστον μια εικονα MNIST.

Αρχικα ο χρηστης εισαγει το μονοπατι του dataset και μετα το μονοπατι του αρχειου αναζητησης και εξοδου,
αφου το προγραμμα παραγει αποτελεσματα εχει την επιλογη η να τερματισει το προγραμμα η να επαναλαβει την αναζητηση με αλλο αρχειο.

Προαιρετικα ο χρηστης μπορει να δωσει στη γραμμη εντολων το πληθος k των lsh συναρτησεων, το πληθος L των των πινακων κατακερματισμου,
τον ακεραιο Ν των πλησιεστερων γειτωνων οπως και τον δεκαδικο R ως ακτινα αναζητησης.Διαφορετικα μενουν οι default τιμες κ=4,L=5,Ν=1,R=10000.
Ο χρηστης αμα δεν εχει δωσει τα αρχεια στη γραμμη εντολων θα τα εισαγει κατα τη διαρκεια του προγραμματος.

Επισης για τον υπερκυβο μπορει να δεινει ο χρηστης τη διασταση κ που προβαλονται τα σημεια, το μεγιστο σημειων που θα ελεχθουν Μ,
το μεγιστο πληθος κορυφων probes, ο ακεραιος N πληθος των γειτονων και ο R της ακτινας αναζητησης.Διαφορετικα μενουν οι default τιμες κ=14,Μ=10,probes=2,Ν=1,R=10000.
Ο χρηστης αμα δεν εχει δωσει τα αρχεια στη γραμμη εντολων θα τα εισαγει κατα τη διαρκεια του προγραμματος.

- Τα αρχεια εισοδου και αναζητησης θα μπορουν να δινονται και μεσω παραμετρων στη γραμμη εντολων.
Οποτε η εκτελεση θα γινεται μεσω της εντολης:

$./lsh -d <input file> -q <query file> -k <int> -L <int> -o <output file> -N
<number of nearest> -R <radious>

$./cube -d <input file> -q <query file> -k <int> -M <int> -probes <int> -o <output file> -N
<number of nearest> -R <radious>